\documentclass{article}
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}
\title{Lecture Notes 2}
\author{}
\date{\today}
\maketitle
\section{第二章\ 贝尔曼公式}
\subsection{状态值及其定义}
\textcolor{blue}{\kaishu*数学直观：return为什么是重要的？}

\textbf{return}可以用来量化并评估某个policy是否是“好的”

v为状态值，r为当前状态的reward，P为与状态空间有关的矩阵，表明不同状态之间的关系

\textbf{状态值}：state value，是在采用某策略的条件下，某个状态出发得到的return的数学期望
\[
v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]
\]
\textcolor{blue}{\kaishu*状态值和return的区别：return是根据某一条trajectory得到的确定的值，而状态值是一个符合概率分布的随机变量}
\subsection{贝尔曼公式的数学推导过程}
\textbf{贝尔曼公式的一般形式}：
$$v_{\pi}(s) = \mathbb{E}[G_t | S_t = s]$$

\begin{align*}
v_{\pi}(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \textcolor{blue}{\mathbb{E}[R_{t+1} | S_t = s]} + \gamma \textcolor{blue}{\mathbb{E}[G_{t+1} | S_t = s]}
\end{align*}
\begin{align*}
\textcolor{blue}{\mathbb{E}[R_{t+1} | S_t = s]} &= \sum_{a} \pi(a|s) \mathbb{E}[R_{t+1} | S_t = s, A_t = a] \\
&= \sum_{a} \pi(a|s) \sum_{r} p(r|s, a)r
\end{align*}
\begin{align*}
\textcolor{blue}{\mathbb{E}[G_{t+1} | S_t = s]} &= \sum_{s'} \mathbb{E}[G_{t+1} | S_t = s, S_{t+1} = s'] p(s'|s) \\
&= \sum_{s'} \mathbb{E}[G_{t+1} | S_{t+1} = s'] p(s'|s) \\
&= \sum_{s'} v_{\pi}(s') p(s'|s) \\
&= \sum_{s'} v_{\pi}(s') \sum_{a} p(s'|s, a) \pi(a|s)
\end{align*}
\textbf{总结}：
\begin{align*}
\textcolor{red}{v_{\pi}(s)} &= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s], \\
&= \underbrace{\textcolor{blue}{\sum_{a} \pi(a|s) \sum_{r} p(r|s, a)r}}_{\text{mean of immediate rewards}} + \underbrace{\textcolor{blue}{\gamma \sum_{a} \pi(a|s) \sum_{s'} p(s'|s, a) \textcolor{red}{v_{\pi}(s')}}}_{\text{mean of future rewards}}, \\
&= \textcolor{blue}{\sum_{a} \pi(a|s) \left[ \sum_{r} p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a) \textcolor{red}{v_{\pi}(s')} \right]}, \quad \forall s \in \mathcal{S}.
\end{align*}

贝尔曼公式用于计算某个状态的状态值。描述了不同状态的状态值之间的关系

\textcolor{blue}{\kaishu*贝尔曼公式对状态空间的所有状态都适用}

\textcolor{blue}{\kaishu*通过求解状态值，可以用来评估policy（即公式中的$\pi$}）

\textcolor{blue}{\kaishu*公式中的$p(r|s, a)$和$p(s'|s, a)$代表dynamic model（或environment model）}

\textcolor{blue}{\kaishu*通过对比不同策略下同一个状态的状态值，就可以知道相对而言策略的好坏}
\subsection{贝尔曼公式的矩阵-向量形式}
以大小为4的状态空间为例：
\[
\underbrace{
  \begin{bmatrix}
    v_{\pi}(s_1) \\
    v_{\pi}(s_2) \\
    v_{\pi}(s_3) \\
    v_{\pi}(s_4)
  \end{bmatrix}
}_{v_{\pi}}
=
\underbrace{
  \begin{bmatrix}
    r_{\pi}(s_1) \\
    r_{\pi}(s_2) \\
    r_{\pi}(s_3) \\
    r_{\pi}(s_4)
  \end{bmatrix}
}_{r_{\pi}}
+ \gamma
\underbrace{
  \begin{bmatrix}
    p_{\pi}(s_1|s_1) & p_{\pi}(s_2|s_1) & p_{\pi}(s_3|s_1) & p_{\pi}(s_4|s_1) \\
    p_{\pi}(s_1|s_2) & p_{\pi}(s_2|s_2) & p_{\pi}(s_3|s_2) & p_{\pi}(s_4|s_2) \\
    p_{\pi}(s_1|s_3) & p_{\pi}(s_2|s_3) & p_{\pi}(s_3|s_3) & p_{\pi}(s_4|s_3) \\
    p_{\pi}(s_1|s_4) & p_{\pi}(s_2|s_4) & p_{\pi}(s_3|s_4) & p_{\pi}(s_4|s_4)
  \end{bmatrix}
}_{P_{\pi}}
\underbrace{
  \begin{bmatrix}
    v_{\pi}(s_1) \\
    v_{\pi}(s_2) \\
    v_{\pi}(s_3) \\
    v_{\pi}(s_4)
  \end{bmatrix}
}_{v_{\pi}}.
\]
\subsection{通过贝尔曼公式求解状态值}
闭式解：$v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}$

\textcolor{blue}{\kaishu*需要求矩阵的逆，在维数较大的时候不易求解}

迭代解法：
$$v_{k+1} = r_{\pi} + \gamma P_{\pi} v_k$$
$$v_k \to v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}, \quad k \to \infty$$
{\kaishu*证明如下：}

将误差定义为 $\delta_k = v_k - v_{\pi}$。我们只需证明 $\delta_k \to 0$。
将 $v_{k+1} = \delta_{k+1} + v_{\pi}$ 和 $v_k = \delta_k + v_{\pi}$ 代入 $v_{k+1} = r_{\pi} + \gamma P_{\pi} v_k$ 可得：
\[
\delta_{k+1} + v_{\pi} = r_{\pi} + \gamma P_{\pi}(\delta_k + v_{\pi}),
\]
上式可以改写为：
\[
\delta_{k+1} = -v_{\pi} + r_{\pi} + \gamma P_{\pi} \delta_k + \gamma P_{\pi} v_{\pi} = \gamma P_{\pi} \delta_k.
\]
（注：这里的化简利用了贝尔曼方程 $v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi}$，所以 $-v_{\pi} + r_{\pi} + \gamma P_{\pi} v_{\pi} = 0$）

因此，通过迭代展开，我们得到：
\[
\delta_{k+1} = \gamma P_{\pi} \delta_k = \gamma^2 P_{\pi}^2 \delta_{k-1} = \dots = \gamma^{k+1} P_{\pi}^{k+1} \delta_0.
\]
注意到 $0 \le P_{\pi}^k \le 1$，这意味着对于任意 $k=0, 1, 2, \dots$，$P_{\pi}^k$ 矩阵中的每一个元素都不大于1。这是因为 $P_{\pi}^k \mathbf{1} = \mathbf{1}$，其中 $\mathbf{1} = [1, \dots, 1]^T$ 是一个全为1的向量。另一方面，由于折扣因子 $\gamma < 1$，我们知道当 $k \to \infty$ 时，$\gamma^k \to 0$。因此，$\delta_{k+1} = \gamma^{k+1} P_{\pi}^{k+1} \delta_0 \to 0$。

这就证明了 $v_k$ 会收敛到 $v_{\pi}$。 
\subsection{行动值（action value）}
定义：$q_{\pi}(s, a) = \mathbb{E} \left[ G_t \mid S_t = s,\, A_t = a \right]$

与状态值的联系：$\textcolor{red}{v_{\pi}(s)} = \sum_{a} \pi(a \mid s) \, \textcolor{red}{q_{\pi}(s, a)}$
根据状态值的计算推导出行动值：$\textcolor{red}{q_{\pi}(s, a)} = \sum_{r} p(r|s, a)r + \gamma \sum_{s'} p(s'|s, a) \textcolor{red}{v_{\pi}(s')}$
\end{document}
